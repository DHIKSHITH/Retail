{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4502c035-1df1-4b97-a5a6-08e099b4de1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60a640ef-7ec4-4445-88b6-97abbf89788b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"s3://dhikshith-source-files/retailer_data_sources/\"\n",
    "\n",
    "customer_path = f\"{base_path}/customer_data.csv\"\n",
    "product_path  = f\"{base_path}/product_data.csv\"\n",
    "sales_path    = f\"{base_path}/sales_returns_data.csv\"\n",
    "payment_path  = f\"{base_path}/card_payment_refund_data.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "614ee0d9-9eab-4304-a19c-a57af74e5f21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customer = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"false\")\n",
    "    .csv(customer_path)\n",
    ")\n",
    "\n",
    "display(df_customer)\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df_customer_clean = (\n",
    "    df_customer\n",
    "    .withColumn(\"customer_id\", col(\"customer_id\").cast(StringType()))\n",
    "    .withColumn(\"customer_name\", col(\"customer_name\").cast(StringType()))\n",
    "    .withColumn(\"email\", col(\"email\").cast(StringType()))\n",
    "    .withColumn(\"phone\", col(\"phone\").cast(StringType()))\n",
    "    .withColumn(\"gender\", col(\"gender\").cast(StringType()))\n",
    "    .withColumn(\"age\", col(\"age\").cast(IntegerType()))\n",
    "    .withColumn(\"region\", col(\"region\").cast(StringType()))\n",
    "    .withColumn(\"city\", col(\"city\").cast(StringType()))\n",
    "    .withColumn(\"signup_date\", col(\"signup_date\").cast(DateType()))\n",
    "    .withColumn(\"ingestion_ts\", current_timestamp())\n",
    ")\n",
    "df_customer_clean.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"retailer_sales.bronze.customer_data_raw\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f83883df-a2bb-465c-9b1f-ebc6b2614b12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_product = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"false\")\n",
    "    .csv(product_path)\n",
    ")\n",
    "\n",
    "display(df_product)\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df_product_clean = (\n",
    "    df_product\n",
    "    .withColumn(\"product_id\", col(\"product_id\").cast(StringType()))\n",
    "    .withColumn(\"product_name\", col(\"product_name\").cast(StringType()))\n",
    "    .withColumn(\"sku\", col(\"sku\").cast(StringType()))\n",
    "    .withColumn(\"category\", col(\"category\").cast(StringType()))\n",
    "    .withColumn(\"supplier\", col(\"supplier\").cast(StringType()))\n",
    "    .withColumn(\"cost_price\", col(\"cost_price\").cast(DoubleType()))\n",
    "    .withColumn(\"selling_price\", col(\"selling_price\").cast(DoubleType()))\n",
    "    .withColumn(\"ingestion_ts\", current_timestamp())\n",
    ")\n",
    "df_product_clean.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"retailer_sales.bronze.product_data_raw\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1489c92-fc90-451c-9bcf-8c2a8d4e8d8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sales = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"false\")\n",
    "    .csv(sales_path)\n",
    ")\n",
    "\n",
    "display(df_sales)\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df_sales_clean = (\n",
    "    df_sales\n",
    "    .withColumn(\"order_id\", col(\"order_id\").cast(StringType()))\n",
    "    .withColumn(\"order_date\", to_date(col(\"order_date\"), \"yyyy-MM-dd\"))\n",
    "    .withColumn(\"customer_id\", col(\"customer_id\").cast(StringType()))\n",
    "    .withColumn(\"product_id\", col(\"product_id\").cast(StringType()))\n",
    "    .withColumn(\"quantity\", col(\"quantity\").cast(IntegerType()))\n",
    "    .withColumn(\"unit_price\", col(\"unit_price\").cast(DoubleType()))\n",
    "    .withColumn(\"total_amount\", col(\"total_amount\").cast(DoubleType()))\n",
    "    .withColumn(\"order_status\", col(\"order_status\").cast(StringType()))\n",
    "    .withColumn(\"return_flag\", col(\"return_flag\").cast(StringType()))  # Y / N\n",
    "    .withColumn(\"region\", col(\"region\").cast(StringType()))\n",
    "    .withColumn(\"payment_mode\", col(\"payment_mode\").cast(StringType()))\n",
    "    .withColumn(\"ingestion_ts\", current_timestamp())\n",
    ")\n",
    "df_sales_clean.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"retailer_sales.bronze.sales_returns_raw\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9ff35d-eb0d-48dc-8929-78ff5065eb0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_payment = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"false\")\n",
    "    .csv(payment_path)\n",
    ")\n",
    "\n",
    "display(df_payment)\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df_payment_clean = (\n",
    "    df_payment\n",
    "    .withColumn(\"transaction_id\", col(\"transaction_id\").cast(StringType()))\n",
    "    .withColumn(\"order_id\", col(\"order_id\").cast(StringType()))\n",
    "    .withColumn(\"transaction_type\", col(\"transaction_type\").cast(StringType()))  # PAYMENT / REFUND\n",
    "    .withColumn(\"amount\", col(\"amount\").cast(DoubleType()))\n",
    "    .withColumn(\"transaction_date\", to_date(col(\"transaction_date\"), \"yyyy-MM-dd\"))\n",
    "    .withColumn(\"payment_mode\", col(\"payment_mode\").cast(StringType()))\n",
    "    .withColumn(\"ingestion_ts\", current_timestamp())\n",
    ")\n",
    "df_payment_clean.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"retailer_sales.bronze.card_payment_refund_raw\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12212b88-4eef-45f3-b595-43f8c945982f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM retailer_sales.bronze.customer_data_raw\").show()\n",
    "spark.sql(\"SELECT COUNT(*) FROM retailer_sales.bronze.product_data_raw\").show()\n",
    "spark.sql(\"SELECT COUNT(*) FROM retailer_sales.bronze.sales_returns_raw\").show()\n",
    "spark.sql(\"SELECT COUNT(*) FROM retailer_sales.bronze.card_payment_refund_raw\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8218918659884346,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
